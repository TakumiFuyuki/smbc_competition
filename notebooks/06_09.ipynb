{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Notebookファイルからの相対パスでCSVファイルのパスを指定\n",
    "csv_file_path = '../data/train.csv'\n",
    "\n",
    "# ファイルの存在確認\n",
    "if not os.path.exists(csv_file_path):\n",
    "    print(f\"エラー: ファイルが見つかりません。パスを確認してください: {os.path.abspath(csv_file_path)}\")\n",
    "else:\n",
    "    try:\n",
    "        # CSVファイルをDataFrameに読み込む\n",
    "        # 'time'列 (0番目の列) をインデックスに指定し、同時に日付型としてパースする\n",
    "        df_train = pd.read_csv(csv_file_path, index_col='time', parse_dates=['time'])\n",
    "        # または index_col=0, parse_dates=[0] でも同じ結果になります。\n",
    "\n",
    "        print(\"DataFrameの読み込みと'time'列のインデックス化・日付型変換が完了しました。\")\n",
    "\n",
    "        # 読み込んだDataFrameの最初の5行を表示して確認\n",
    "        # print(\"\\nDataFrameの先頭5行:\")\n",
    "        # print(df_train.head())\n",
    "\n",
    "        # DataFrameのインデックス情報を確認\n",
    "        # print(\"\\nDataFrameのインデックス情報:\")\n",
    "        # print(df_train.index)\n",
    "\n",
    "        # DataFrameの基本情報を表示 (time列がインデックスになっていることを確認)\n",
    "        # print(\"\\nDataFrameの情報:\")\n",
    "        # df_train.info()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"エラー: CSVファイルの読み込み中に問題が発生しました: {e}\")\n",
    "\n",
    "# この時点で df_train は 'time' 列を DatetimeIndex として持っています。\n",
    "# この後の欠損値処理などは、この df_train に対して行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 欠損値補完 ---\n",
    "# 以前の分類に基づく連続変数のリスト\n",
    "continuous_cols = [\n",
    "    'generation_biomass', 'generation_fossil_brown_coal/lignite',\n",
    "    'generation_fossil_gas', 'generation_fossil_hard_coal', 'generation_fossil_oil',\n",
    "    'generation_hydro_pumped_storage_consumption', 'generation_hydro_run_of_river_and_poundage',\n",
    "    'generation_hydro_water_reservoir', 'generation_nuclear', 'generation_other',\n",
    "    'generation_other_renewable', 'generation_solar', 'generation_waste',\n",
    "    'generation_wind_onshore', 'total_load_actual'\n",
    "]\n",
    "\n",
    "# 1. 0で補完するのが適切なカラム\n",
    "cols_to_fill_zero = [col for col in df_train.columns if 'rain' in col or 'snow' in col]\n",
    "cols_to_fill_zero.append('generation_solar')\n",
    "\n",
    "print(f\"0で補完するカラム: {cols_to_fill_zero}\")\n",
    "for col in cols_to_fill_zero:\n",
    "    if col in df_train.columns:\n",
    "        df_train[col] = df_train[col].fillna(0)\n",
    "\n",
    "# 2. 残りの連続変数に対して前方補完\n",
    "# continuous_colsは以前定義した連続変数のリスト\n",
    "remaining_continuous_cols = [col for col in continuous_cols if col not in cols_to_fill_zero]\n",
    "\n",
    "print(\"前方補完（ffill）を実行するカラム（一部抜粋）:\", remaining_continuous_cols[:5])\n",
    "for col in remaining_continuous_cols:\n",
    "    if col in df_train.columns:\n",
    "        df_train[col] = df_train[col].fillna(method='ffill')\n",
    "\n",
    "# 3. それでも先頭に残った欠損を後方補完（bfill）で埋める\n",
    "# （データの先頭から欠損が続いている場合、ffillでは埋まらないため）\n",
    "df_train = df_train.fillna(method='bfill')\n",
    "\n",
    "print(\"\\n欠損値の補完が完了しました。\")\n",
    "# 補完後に欠損がないか確認\n",
    "print(\"補完後の欠損値の数:\\n\", df_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# このコードを実行する前に、df_trainがロードされている必要があります。\n",
    "# 例: df_train = pd.read_csv('your_data.csv', index_col='time', parse_dates=True)\n",
    "\n",
    "# --- ステップ①: 風向をサイン・コサインに変換 ---\n",
    "print(\"ステップ①: 風向の処理を開始します...\")\n",
    "cities = ['valencia', 'madrid', 'bilbao', 'barcelona', 'seville']\n",
    "original_wind_deg_cols = []\n",
    "\n",
    "for city in cities:\n",
    "    wind_deg_col = f'{city}_wind_deg'\n",
    "    original_wind_deg_cols.append(wind_deg_col)\n",
    "\n",
    "    # 度数をラジアンに変換\n",
    "    radians = np.deg2rad(df_train[wind_deg_col])\n",
    "\n",
    "    # 新しいサイン・コサイン特徴量を作成\n",
    "    df_train[f'{city}_wind_deg_cos'] = np.cos(radians)\n",
    "    df_train[f'{city}_wind_deg_sin'] = np.sin(radians)\n",
    "\n",
    "# 元の風向きカラムを削除\n",
    "df_train = df_train.drop(columns=original_wind_deg_cols)\n",
    "print(f\"元の風向カラム {original_wind_deg_cols} を削除し、サイン・コサインカラムを10個追加しました。\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- ステップ②: 連続変数の標準化 ---\n",
    "print(\"ステップ②: 連続変数の標準化を開始します...\")\n",
    "\n",
    "# 以前の分類に基づく連続変数のリスト\n",
    "# continuous_cols = [\n",
    "#     'generation_biomass', 'generation_fossil_brown_coal/lignite',\n",
    "#     'generation_fossil_gas', 'generation_fossil_hard_coal', 'generation_fossil_oil',\n",
    "#     'generation_hydro_pumped_storage_consumption', 'generation_hydro_run_of_river_and_poundage',\n",
    "#     'generation_hydro_water_reservoir', 'generation_nuclear', 'generation_other',\n",
    "#     'generation_other_renewable', 'generation_solar', 'generation_waste',\n",
    "#     'generation_wind_onshore', 'total_load_actual'\n",
    "# ]\n",
    "for city in cities:\n",
    "    continuous_cols.extend([\n",
    "        f'{city}_temp', f'{city}_temp_min', f'{city}_temp_max', f'{city}_pressure',\n",
    "        f'{city}_humidity', f'{city}_wind_speed', f'{city}_rain_1h', f'{city}_rain_3h',\n",
    "        f'{city}_snow_3h', f'{city}_clouds_all'\n",
    "    ])\n",
    "\n",
    "# ステップ①で追加したサイン・コサインカラムもリストに追加\n",
    "for city in cities:\n",
    "    continuous_cols.extend([f'{city}_wind_deg_cos', f'{city}_wind_deg_sin'])\n",
    "\n",
    "# 欠損値があるとエラーになるため、対象カラムを絞り込み\n",
    "# （実際には、標準化の前に欠損値処理を行うのが望ましいです）\n",
    "cols_to_scale = [col for col in continuous_cols if col in df_train.columns and df_train[col].isnull().sum() == 0]\n",
    "print(f\"標準化の対象となるカラム数: {len(cols_to_scale)}\")\n",
    "\n",
    "# 標準化の実行\n",
    "scaler = StandardScaler()\n",
    "df_train[cols_to_scale] = scaler.fit_transform(df_train[cols_to_scale])\n",
    "\n",
    "print(\"連続変数の標準化が完了しました。\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 処理後のデータフレームの先頭を表示して確認\n",
    "print(\"処理後のデータフレームの先頭5行:\")\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 欠損値補完 ---\n",
    "\n",
    "# # 1. 0で補完するのが適切なカラム\n",
    "# cols_to_fill_zero = [col for col in df_train.columns if 'rain' in col or 'snow' in col]\n",
    "# cols_to_fill_zero.append('generation_solar')\n",
    "\n",
    "# print(f\"0で補完するカラム: {cols_to_fill_zero}\")\n",
    "# for col in cols_to_fill_zero:\n",
    "#     if col in df_train.columns:\n",
    "#         df_train[col] = df_train[col].fillna(0)\n",
    "\n",
    "# # 2. 残りの連続変数に対して前方補完\n",
    "# # continuous_colsは以前定義した連続変数のリスト\n",
    "# remaining_continuous_cols = [col for col in continuous_cols if col not in cols_to_fill_zero]\n",
    "\n",
    "# print(\"前方補完（ffill）を実行するカラム（一部抜粋）:\", remaining_continuous_cols[:5])\n",
    "# for col in remaining_continuous_cols:\n",
    "#     if col in df_train.columns:\n",
    "#         df_train[col] = df_train[col].fillna(method='ffill')\n",
    "\n",
    "# # 3. それでも先頭に残った欠損を後方補完（bfill）で埋める\n",
    "# # （データの先頭から欠損が続いている場合、ffillでは埋まらないため）\n",
    "# df_train = df_train.fillna(method='bfill')\n",
    "\n",
    "# print(\"\\n欠損値の補完が完了しました。\")\n",
    "# # 補完後に欠損がないか確認\n",
    "# print(\"補完後の欠損値の数:\\n\", df_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# このコードを実行する前に、df_trainが前回の処理を終えた状態でロードされている必要があります。\n",
    "\n",
    "print(\"処理前のデータフレームの形状:\", df_train.shape)\n",
    "print(\"処理前のカラム（一部）:\", df_train.columns[:10].tolist(), \"...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- ステップ①: weather_descriptionカラムを削除 ---\n",
    "print(\"ステップ①: weather_descriptionカラムの削除を開始します...\")\n",
    "cities = ['valencia', 'madrid', 'bilbao', 'barcelona', 'seville']\n",
    "description_cols_to_drop = [f'{city}_weather_description' for city in cities]\n",
    "\n",
    "# 存在するカラムのみを削除対象とする\n",
    "existing_cols_to_drop = [col for col in description_cols_to_drop if col in df_train.columns]\n",
    "if existing_cols_to_drop:\n",
    "    df_train = df_train.drop(columns=existing_cols_to_drop)\n",
    "    print(f\"削除されたカラム: {existing_cols_to_drop}\")\n",
    "else:\n",
    "    print(\"削除対象のdescriptionカラムは見つかりませんでした。\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- ステップ②: 残りのカテゴリ変数にOne-Hot Encodingを適用 ---\n",
    "print(\"ステップ②: One-Hot Encodingを開始します...\")\n",
    "\n",
    "# One-Hot Encodingの対象となるカラムのリストを作成\n",
    "categorical_cols_to_encode = []\n",
    "for city in cities:\n",
    "    categorical_cols_to_encode.extend([\n",
    "        f'{city}_weather_id',\n",
    "        f'{city}_weather_main',\n",
    "        f'{city}_weather_icon'\n",
    "    ])\n",
    "\n",
    "# df_trainに存在するカラムのみを対象とする\n",
    "existing_categorical_cols = [col for col in categorical_cols_to_encode if col in df_train.columns]\n",
    "\n",
    "if existing_categorical_cols:\n",
    "    # weather_idは数値型のため、カテゴリとして正しく扱われるように文字列に変換\n",
    "    for col in existing_categorical_cols:\n",
    "        if 'weather_id' in col:\n",
    "            df_train[col] = df_train[col].astype(str)\n",
    "    \n",
    "    print(f\"One-Hot Encoding対象のカラム: {existing_categorical_cols}\")\n",
    "    \n",
    "    # One-Hot Encodingを実行\n",
    "    # prefixを指定して、どの元のカラムから来た特徴量か分かりやすくします。\n",
    "    # 元のカテゴリカルカラムは自動的に削除されます。\n",
    "    df_train = pd.get_dummies(df_train, columns=existing_categorical_cols)\n",
    "    \n",
    "    print(\"One-Hot Encodingが完了しました。\")\n",
    "else:\n",
    "    print(\"One-Hot Encoding対象のカラムは見つかりませんでした。\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"全ての処理が完了しました。\")\n",
    "print(\"最終的なデータフレームの形状:\", df_train.shape)\n",
    "print(\"処理後のカラム（一部）:\", df_train.columns[:10].tolist(), \"...\")\n",
    "print(\"\\n処理後のデータフレームの先頭5行:\")\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"price_actual\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# df_train が前処理済みの状態で存在すると仮定します。\n",
    "# 例：df_train = pd.read_csv(...) などでロードされ、\n",
    "# 欠損値補完、標準化、One-Hotエンコーディングが完了している状態。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_sequences(input_data, target_column, timesteps):\n",
    "#     \"\"\"\n",
    "#     2Dの時系列データフレームから、LSTM用の3Dシーケンスデータを作成する関数\n",
    "#     \"\"\"\n",
    "#     X_list, y_list = [], []\n",
    "    \n",
    "#     # 説明変数と目的変数に分離\n",
    "#     # One-Hotエンコーディングにより目的変数の位置が変わっている可能性があるため、名前で指定\n",
    "#     y = input_data[target_column].values\n",
    "#     X_data = input_data.drop(columns=target_column).values\n",
    "    \n",
    "#     for i in range(len(X_data) - timesteps):\n",
    "#         X_list.append(X_data[i:i+timesteps])\n",
    "#         y_list.append(y[i+timesteps])\n",
    "        \n",
    "#     return np.array(X_list), np.array(y_list)\n",
    "\n",
    "# # ハイパーパラメータ\n",
    "# TIMESTEPS = 24\n",
    "# TARGET_COLUMN = 'price_actual' # 目的変数のカラム名\n",
    "\n",
    "# # ★★★ 修正箇所：ここから ★★★\n",
    "# # bool型をint型に変換して、データ型を数値に統一します\n",
    "# bool_cols = df_train.select_dtypes(include=['bool']).columns\n",
    "# df_train[bool_cols] = df_train[bool_cols].astype(int)\n",
    "# print(\"bool型をint型に変換しました。\")\n",
    "# # ★★★ 修正箇所：ここまで ★★★\n",
    "\n",
    "# print(\"シーケンスデータを作成中...\")\n",
    "# X_seq, y_seq = create_sequences(df_train, TARGET_COLUMN, TIMESTEPS)\n",
    "\n",
    "# print(f\"X_seq shape: {X_seq.shape}\") # -> (サンプル数, 24, 特徴量数)\n",
    "# print(f\"y_seq shape: {y_seq.shape}\") # -> (サンプル数,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # シーケンスを作成する前のdf_trainの状態を確認します。\n",
    "# # このコードを、`create_sequences`関数を呼び出す前に追加してください。\n",
    "\n",
    "# print(\"データフレームのデータ型情報を表示します:\")\n",
    "# df_train.info()\n",
    "\n",
    "# print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# # object型のカラムのみをリストアップ\n",
    "# object_cols = df_train.select_dtypes(include=['object']).columns\n",
    "# if not object_cols.empty:\n",
    "#     print(f\"エラーの原因となっている可能性のある'object'型のカラム: {object_cols.tolist()}\")\n",
    "# else:\n",
    "#     print(\"'object'型のカラムは見つかりませんでした。他の原因が考えられます。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 訓練データと検証データの分割（例: 80%を訓練、20%を検証）\n",
    "# train_size = int(len(X_seq) * 0.8)\n",
    "# X_train, X_val = X_seq[:train_size], X_seq[train_size:]\n",
    "# y_train, y_val = y_seq[:train_size], y_seq[train_size:]\n",
    "\n",
    "# # PyTorchのテンソルに変換\n",
    "# X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "# X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "# y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# # DatasetとDataLoaderの作成\n",
    "# BATCH_SIZE = 64 # バッチサイズも後で調整するハイパーパラメータ\n",
    "\n",
    "# train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# print(\"DataLoaderの準備が完了しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTMModel(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, dropout_prob):\n",
    "#         super(LSTMModel, self).__init__()\n",
    "#         self.lstm = nn.LSTM(\n",
    "#             input_size=input_size,\n",
    "#             hidden_size=hidden_size,\n",
    "#             num_layers=num_layers,\n",
    "#             batch_first=True,\n",
    "#             dropout=dropout_prob if num_layers > 1 else 0 # 1層の場合はdropoutは適用されない\n",
    "#         )\n",
    "#         self.dropout = nn.Dropout(dropout_prob)\n",
    "#         self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # LSTM層からの出力を取得\n",
    "#         # h_n, c_n (最後の隠れ状態とセル状態) は今回は使用しない\n",
    "#         lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "#         # 最後のタイムステップの出力のみを選択\n",
    "#         last_hidden_state = lstm_out[:, -1, :]\n",
    "        \n",
    "#         # ドロップアウト層\n",
    "#         out = self.dropout(last_hidden_state)\n",
    "        \n",
    "#         # 全結合層（出力層）\n",
    "#         out = self.fc(out)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ハイパーパラメータ（Optunaで探索する候補）\n",
    "# INPUT_SIZE = X_train.shape[2] # 特徴量の数\n",
    "# HIDDEN_SIZE = 100\n",
    "# NUM_LAYERS = 2\n",
    "# DROPOUT_PROB = 0.2\n",
    "# LEARNING_RATE = 0.001\n",
    "# NUM_EPOCHS = 20 # まずは少ないエポック数で試す\n",
    "\n",
    "# # デバイスの設定\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(f\"使用デバイス: {device}\")\n",
    "\n",
    "# # モデル、損失関数、オプティマイザのインスタンス化\n",
    "# model = LSTMModel(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT_PROB).to(device)\n",
    "# loss_fn = nn.MSELoss() # 平均二乗誤差\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# print(\"モデルの学習を開始します...\")\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "#     # --- 訓練モード ---\n",
    "#     model.train()\n",
    "#     total_train_loss = 0\n",
    "#     for inputs, labels in train_loader:\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         predictions = model(inputs)\n",
    "#         loss = loss_fn(predictions, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         total_train_loss += loss.item()\n",
    "    \n",
    "#     avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "#     # --- 検証モード ---\n",
    "#     model.eval()\n",
    "#     total_val_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in val_loader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             predictions = model(inputs)\n",
    "#             loss = loss_fn(predictions, labels)\n",
    "#             total_val_loss += loss.item()\n",
    "            \n",
    "#     avg_val_loss = total_val_loss / len(val_loader)\n",
    "    \n",
    "#     print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] | Train Loss: {avg_train_loss:.4f} | Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# --- このコードの前提 ---\n",
    "# df_train: 全ての前処理が完了した訓練データ (DataFrame)\n",
    "# LSTMModel: 定義済みのモデルクラス\n",
    "# create_sequences: 定義済みのシーケンス作成関数\n",
    "# TARGET_COLUMN = 'price_actual'\n",
    "# device = 'cuda' or 'cpu'\n",
    "\n",
    "# --- 1. Optunaの `objective` 関数を定義 ---\n",
    "def objective(trial):\n",
    "    # --- A. 探索対象のハイパーパラメータを定義 ---\n",
    "    # trial.suggest_... で探索範囲を指定します\n",
    "    timesteps = trial.suggest_categorical('timesteps', [24, 48, 72])\n",
    "    hidden_size = trial.suggest_int('hidden_size', 32, 128, log=True)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    dropout_prob = trial.suggest_float('dropout_prob', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "\n",
    "    # --- B. データ準備 (trialごとにtimestepsが変わるため、毎回作成) ---\n",
    "    X_seq, y_seq = create_sequences(df_train, TARGET_COLUMN, timesteps)\n",
    "    \n",
    "    train_size = int(len(X_seq) * 0.8)\n",
    "    X_train, X_val = X_seq[:train_size], X_seq[train_size:]\n",
    "    y_train, y_val = y_seq[:train_size], y_seq[train_size:]\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # --- C. モデルと学習設定のインスタンス化 ---\n",
    "    input_size = X_train.shape[2]\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, dropout_prob).to(device)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # --- D. モデルの学習と評価 ---\n",
    "    # 最適化中はエポック数を少なめにして、高速に探索します\n",
    "    NUM_EPOCHS_OPTIM = 10\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS_OPTIM):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(inputs)\n",
    "            loss = loss_fn(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # 最後の検証ロスを評価値とする\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            predictions = model(inputs)\n",
    "            loss = loss_fn(predictions, labels)\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    \n",
    "    # --- E. 評価値を返す ---\n",
    "    # Optunaはこの戻り値を最小化しようとします\n",
    "    return avg_val_loss\n",
    "\n",
    "\n",
    "# --- 2. Optunaの学習（最適化）を実行 ---\n",
    "# `direction='minimize'`で、objective関数の戻り値（検証ロス）を最小化するよう指示\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "# n_trialsで試行回数を指定。まずは30-50回程度から試すのがおすすめです。\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "\n",
    "# --- 3. 結果の確認 ---\n",
    "print(\"\\n最適化が完了しました。\")\n",
    "print(\"=\"*50)\n",
    "print(\"最高のトライアル:\")\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(f\"  Value (最小検証ロス): {best_trial.value}\")\n",
    "print(\"  Params (最高のハイパーパラメータ): \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各試行の履歴をプロット\n",
    "optuna.visualization.plot_optimization_history(study).show()\n",
    "\n",
    "# 各ハイパーパラメータの重要度をプロット\n",
    "optuna.visualization.plot_param_importances(study).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# --- このコードの前提 ---\n",
    "# study: Optunaの最適化が完了したstudyオブジェクト\n",
    "# df_train: 全ての前処理が完了した訓練データ (DataFrame)\n",
    "# LSTMModel: 定義済みのモデルクラス\n",
    "# create_sequences: 定義済みのシーケンス作成関数\n",
    "# TARGET_COLUMN = 'price_actual'\n",
    "# device = 'cuda' or 'cpu'\n",
    "\n",
    "# --- 1. Optunaで見つけた最高のハイパーパラメータを取得 ---\n",
    "best_params = study.best_params\n",
    "print(\"Optunaによって見つかった最高のハイパーパラメータ:\")\n",
    "print(best_params)\n",
    "\n",
    "# 各パラメータを変数に格納\n",
    "FINAL_TIMESTEPS = best_params['timesteps']\n",
    "FINAL_HIDDEN_SIZE = best_params['hidden_size']\n",
    "FINAL_NUM_LAYERS = best_params['num_layers']\n",
    "FINAL_DROPOUT_PROB = best_params['dropout_prob']\n",
    "FINAL_LEARNING_RATE = best_params['learning_rate']\n",
    "FINAL_BATCH_SIZE = best_params['batch_size']\n",
    "\n",
    "\n",
    "# --- 2. 全ての訓練データを使用して最終的な学習データを準備 ---\n",
    "print(\"\\n最終モデル学習のため、全ての訓練データでシーケンスを作成中...\")\n",
    "X_final_train, y_final_train = create_sequences(df_train, TARGET_COLUMN, FINAL_TIMESTEPS)\n",
    "\n",
    "X_final_train_tensor = torch.tensor(X_final_train, dtype=torch.float32)\n",
    "y_final_train_tensor = torch.tensor(y_final_train, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "final_train_dataset = TensorDataset(X_final_train_tensor, y_final_train_tensor)\n",
    "final_train_loader = DataLoader(final_train_dataset, batch_size=FINAL_BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"最終的な学習用シーケンスの形状: {X_final_train_tensor.shape}\")\n",
    "\n",
    "\n",
    "# --- 3. 最終モデルの構築と学習の実行 ---\n",
    "# 最高のパラメータでモデルを再構築\n",
    "final_model = LSTMModel(\n",
    "    input_size=X_final_train.shape[2],\n",
    "    hidden_size=FINAL_HIDDEN_SIZE,\n",
    "    num_layers=FINAL_NUM_LAYERS,\n",
    "    dropout_prob=FINAL_DROPOUT_PROB\n",
    ").to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(final_model.parameters(), lr=FINAL_LEARNING_RATE)\n",
    "\n",
    "FINAL_NUM_EPOCHS = 50 # 十分なエポック数で学習\n",
    "print(f\"\\n最終モデルの学習を開始します... (Epochs: {FINAL_NUM_EPOCHS})\")\n",
    "\n",
    "final_model.train() # モデルを訓練モードに設定\n",
    "for epoch in range(FINAL_NUM_EPOCHS):\n",
    "    total_loss = 0\n",
    "    for inputs, labels in final_train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = final_model(inputs)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # 5エポックごとに進捗を表示\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        avg_loss = total_loss / len(final_train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{FINAL_NUM_EPOCHS}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "# --- 4. 学習済みモデルの保存 ---\n",
    "model_save_path = 'final_lstm_model.pth'\n",
    "torch.save(final_model.state_dict(), model_save_path)\n",
    "\n",
    "print(\"\\n学習が完了し、最終モデルを'{model_save_path}'として保存しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(input_data, target_column, timesteps):\n",
    "    \"\"\"\n",
    "    2Dの時系列データフレームから、LSTM用の3Dシーケンスデータを作成する関数\n",
    "    \"\"\"\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    # 説明変数と目的変数に分離\n",
    "    # One-Hotエンコーディングにより目的変数の位置が変わっている可能性があるため、名前で指定\n",
    "    y = input_data[target_column].values\n",
    "    X_data = input_data.drop(columns=target_column).values\n",
    "\n",
    "    for i in range(len(X_data) - timesteps):\n",
    "        X_list.append(X_data[i:i+timesteps])\n",
    "        y_list.append(y[i+timesteps])\n",
    "\n",
    "    return np.array(X_list), np.array(y_list)\n",
    "\n",
    "# ハイパーパラメータ\n",
    "TIMESTEPS = 48\n",
    "TARGET_COLUMN = 'price_actual' # 目的変数のカラム名\n",
    "\n",
    "# ★★★ 修正箇所：ここから ★★★\n",
    "# bool型をint型に変換して、データ型を数値に統一します\n",
    "bool_cols = df_train.select_dtypes(include=['bool']).columns\n",
    "df_train[bool_cols] = df_train[bool_cols].astype(int)\n",
    "print(\"bool型をint型に変換しました。\")\n",
    "# ★★★ 修正箇所：ここまで ★★★\n",
    "\n",
    "print(\"シーケンスデータを作成中...\")\n",
    "X_seq, y_seq = create_sequences(df_train, TARGET_COLUMN, TIMESTEPS)\n",
    "\n",
    "print(f\"X_seq shape: {X_seq.shape}\") # -> (サンプル数, 24, 特徴量数)\n",
    "print(f\"y_seq shape: {y_seq.shape}\") # -> (サンプル数,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# シーケンスを作成する前のdf_trainの状態を確認します。\n",
    "# このコードを、`create_sequences`関数を呼び出す前に追加してください。\n",
    "\n",
    "print(\"データフレームのデータ型情報を表示します:\")\n",
    "df_train.info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# object型のカラムのみをリストアップ\n",
    "object_cols = df_train.select_dtypes(include=['object']).columns\n",
    "if not object_cols.empty:\n",
    "    print(f\"エラーの原因となっている可能性のある'object'型のカラム: {object_cols.tolist()}\")\n",
    "else:\n",
    "    print(\"'object'型のカラムは見つかりませんでした。他の原因が考えられます。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データと検証データの分割（例: 80%を訓練、20%を検証）\n",
    "train_size = int(len(X_seq) * 0.8)\n",
    "X_train, X_val = X_seq[:train_size], X_seq[train_size:]\n",
    "y_train, y_val = y_seq[:train_size], y_seq[train_size:]\n",
    "\n",
    "# PyTorchのテンソルに変換\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# DatasetとDataLoaderの作成\n",
    "BATCH_SIZE = 32 # バッチサイズも後で調整するハイパーパラメータ\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"DataLoaderの準備が完了しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_prob):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_prob if num_layers > 1 else 0 # 1層の場合はdropoutは適用されない\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM層からの出力を取得\n",
    "        # h_n, c_n (最後の隠れ状態とセル状態) は今回は使用しない\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # 最後のタイムステップの出力のみを選択\n",
    "        last_hidden_state = lstm_out[:, -1, :]\n",
    "        \n",
    "        # ドロップアウト層\n",
    "        out = self.dropout(last_hidden_state)\n",
    "        \n",
    "        # 全結合層（出力層）\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータ（Optunaで探索する候補）\n",
    "INPUT_SIZE = X_train.shape[2] # 特徴量の数\n",
    "HIDDEN_SIZE = 37\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT_PROB = 0.2020769227005973\n",
    "LEARNING_RATE = 0.0014057516838302802\n",
    "NUM_EPOCHS = 50 # まずは少ないエポック数で試す\n",
    "\n",
    "# デバイスの設定\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用デバイス: {device}\")\n",
    "\n",
    "# モデル、損失関数、オプティマイザのインスタンス化\n",
    "model = LSTMModel(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT_PROB).to(device)\n",
    "loss_fn = nn.MSELoss() # 平均二乗誤差\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"モデルの学習を開始します...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # --- 訓練モード ---\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    # --- 検証モード ---\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            predictions = model(inputs)\n",
    "            loss = loss_fn(predictions, labels)\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] | Train Loss: {avg_train_loss:.4f} | Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Notebookファイルからの相対パスでCSVファイルのパスを指定\n",
    "csv_file_path = '../data/test.csv'\n",
    "\n",
    "# ファイルの存在確認\n",
    "if not os.path.exists(csv_file_path):\n",
    "    print(f\"エラー: ファイルが見つかりません。パスを確認してください: {os.path.abspath(csv_file_path)}\")\n",
    "else:\n",
    "    try:\n",
    "        # CSVファイルをDataFrameに読み込む\n",
    "        # 'time'列 (0番目の列) をインデックスに指定し、同時に日付型としてパースする\n",
    "        df_test_original = pd.read_csv(csv_file_path, index_col='time', parse_dates=['time'])\n",
    "        # または index_col=0, parse_dates=[0] でも同じ結果になります。\n",
    "\n",
    "        print(\"DataFrameの読み込みと'time'列のインデックス化・日付型変換が完了しました。\")\n",
    "\n",
    "        # 読み込んだDataFrameの最初の5行を表示して確認\n",
    "        # print(\"\\nDataFrameの先頭5行:\")\n",
    "        # print(df_test_original.head())\n",
    "\n",
    "        # DataFrameのインデックス情報を確認\n",
    "        # print(\"\\nDataFrameのインデックス情報:\")\n",
    "        # print(df_test_original.index)\n",
    "\n",
    "        # DataFrameの基本情報を表示 (time列がインデックスになっていることを確認)\n",
    "        # print(\"\\nDataFrameの情報:\")\n",
    "        # df_test_original.info()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"エラー: CSVファイルの読み込み中に問題が発生しました: {e}\")\n",
    "\n",
    "# この時点で df_test_original は 'time' 列を DatetimeIndex として持っています。\n",
    "# この後の欠損値処理などは、この df_test_original に対して行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# --- このコードの前提 ---\n",
    "# df_train_original: 前処理前の元の訓練データ（'price_actual'を含む）\n",
    "# df_test_original: 前処理前の元のテストデータ\n",
    "# model: 学習済みのPyTorchモデル\n",
    "# scaler: 訓練データの説明変数のみで学習済みのStandardScaler\n",
    "# continuous_cols: 連続変数のカラム名リスト\n",
    "# TIMESTEPS = 24\n",
    "# BATCH_SIZE = 64\n",
    "# device = 'cuda' or 'cpu'\n",
    "\n",
    "# --- ステップ0: テストデータをクリーンな状態にする ---\n",
    "# 元のテストデータをコピーして、他のセルの影響を排除する\n",
    "df_test = df_test_original.copy()\n",
    "print(\"テストデータの前処理を開始します...\")\n",
    "print(f\"処理前のdf_testの形状: {df_test.shape}\")\n",
    "\n",
    "# --- ステップ1: 欠損値補完 ---\n",
    "print(\"1. 欠損値を補完中...\")\n",
    "cols_to_fill_zero = [col for col in df_test.columns if 'rain' in col or 'snow' in col or 'solar' in col]\n",
    "df_test[cols_to_fill_zero] = df_test[cols_to_fill_zero].fillna(0)\n",
    "df_test = df_test.ffill().bfill() # FutureWarningを避けるため .ffill()/.bfill() に変更\n",
    "\n",
    "# --- ステップ2: 風向の処理と不要カラムの削除 ---\n",
    "print(\"2. 風向をサイン・コサインに変換し、不要なカラムを削除中...\")\n",
    "cities = ['valencia', 'madrid', 'bilbao', 'barcelona', 'seville']\n",
    "original_wind_deg_cols = [f'{city}_wind_deg' for city in cities]\n",
    "for col in original_wind_deg_cols:\n",
    "    if col in df_test.columns:\n",
    "        radians = np.deg2rad(df_test[col])\n",
    "        df_test[f'{col}_cos'] = np.cos(radians)\n",
    "        df_test[f'{col}_sin'] = np.sin(radians)\n",
    "df_test = df_test.drop(columns=original_wind_deg_cols, errors='ignore')\n",
    "description_cols_to_drop = [f'{city}_weather_description' for city in cities]\n",
    "df_test = df_test.drop(columns=description_cols_to_drop, errors='ignore')\n",
    "\n",
    "# --- ステップ3: 標準化 (訓練時のScalerを再利用) ---\n",
    "print(\"3. 訓練時のScalerを再利用して標準化中...\")\n",
    "cols_to_scale_test = [col for col in continuous_cols if col in df_test.columns and col != 'price_actual']\n",
    "df_test[cols_to_scale_test] = scaler.transform(df_test[cols_to_scale_test])\n",
    "\n",
    "# --- ステップ4: One-Hot Encoding とカラムの整合 ---\n",
    "print(\"4. One-Hot Encodingとカラムの整合性を確保中...\")\n",
    "categorical_cols_to_encode = []\n",
    "for city in cities:\n",
    "    categorical_cols_to_encode.extend([f'{city}_weather_id', f'{city}_weather_main', f'{city}_weather_icon'])\n",
    "existing_categorical_cols = [col for col in categorical_cols_to_encode if col in df_test.columns]\n",
    "for col in existing_categorical_cols:\n",
    "    df_test[col] = df_test[col].astype(str)\n",
    "df_test = pd.get_dummies(df_test, columns=existing_categorical_cols)\n",
    "\n",
    "# 訓練データから目的変数を除いたカラムリストを基準にする\n",
    "train_feature_columns = df_train.drop(columns=['price_actual']).columns\n",
    "df_test = df_test.reindex(columns=train_feature_columns, fill_value=0)\n",
    "\n",
    "bool_cols = df_test.select_dtypes(include=['bool']).columns\n",
    "df_test[bool_cols] = df_test[bool_cols].astype(int)\n",
    "print(\"前処理が完了しました。\")\n",
    "\n",
    "# --- これ以降は前回と同様の推論コード ---\n",
    "# (中略：推論と提出ファイル作成のコードをここに続ける)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# --- このステップの前提 ---\n",
    "# df_test: 全ての前処理が完了したテストデータ (DataFrame)\n",
    "# model: 学習済みのPyTorchモデル\n",
    "# device: 'cuda' or 'cpu'\n",
    "# TIMESTEPS = 24 (訓練時と同じ値)\n",
    "\n",
    "# --- ステップ1: テスト用シーケンスデータの作成 ---\n",
    "# ターゲット（正解ラベル）がないため、説明変数Xのみのシーケンスを作成します\n",
    "def create_test_sequences(input_data, timesteps):\n",
    "    X_list = []\n",
    "    X_data = input_data.values\n",
    "    for i in range(len(X_data) - timesteps + 1):\n",
    "        X_list.append(X_data[i:i+timesteps])\n",
    "    return np.array(X_list)\n",
    "\n",
    "print(\"テストデータから予測用のシーケンスを作成中...\")\n",
    "X_test = create_test_sequences(df_test, TIMESTEPS)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "print(f\"作成されたテストシーケンスの形状: {X_test.shape}\")\n",
    "\n",
    "\n",
    "# --- ステップ2: モデルによる予測の実行 ---\n",
    "print(\"モデルによる予測を実行中...\")\n",
    "\n",
    "# テストデータ用のDataLoaderを作成（ラベルは不要）\n",
    "test_dataset = TensorDataset(X_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False) # BATCH_SIZEは訓練時と同じ\n",
    "\n",
    "# モデルを評価モードに切り替え\n",
    "model.eval()\n",
    "\n",
    "# 予測値を保存するリスト\n",
    "all_predictions = []\n",
    "\n",
    "# 勾配計算をオフにして予測\n",
    "with torch.no_grad():\n",
    "    for inputs in test_loader: # inputsはリストなので[0]でテンソルを取り出す\n",
    "        inputs_tensor = inputs[0].to(device)\n",
    "        predictions = model(inputs_tensor)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "\n",
    "# リストを結合して一つのNumpy配列に\n",
    "predictions_np = np.concatenate(all_predictions).flatten() # 1次元配列に変換\n",
    "\n",
    "print(\"予測が完了しました。\")\n",
    "\n",
    "\n",
    "# # --- ステップ3: 提出用CSVファイルの作成 ---\n",
    "# print(\"提出用CSVファイルを作成中...\")\n",
    "\n",
    "# # 予測値に対応するタイムスタンプを取得\n",
    "# # create_test_sequencesでは (len - timesteps + 1) 個のシーケンスが作られる\n",
    "# # しかし、提出形式はdf_testの全期間であるため、予測値の個数とタイムスタンプの個数を合わせる必要がある\n",
    "# # ここでは、タイムスタンプの最後の部分に予測値を割り当てるのが一般的\n",
    "# num_predictions = len(predictions_np)\n",
    "# num_timestamps = len(df_test.index)\n",
    "\n",
    "# # 提出用データフレームを作成\n",
    "# submission_df = pd.DataFrame(index=df_test.index)\n",
    "# submission_df['price_actual'] = np.nan # 一旦NaNで初期化\n",
    "\n",
    "# # 最後の部分に予測値を割り当てる\n",
    "# submission_df['price_actual'].iloc[-num_predictions:] = predictions_np\n",
    "\n",
    "# # 予測できなかった先頭部分を直後の値で埋める（前方補完の逆）\n",
    "# submission_df = submission_df.fillna(method='bfill')\n",
    "\n",
    "# # 提出形式に合わせてCSVファイルとして保存\n",
    "# # header=False とすることで、カラム名を入れずに出力します\n",
    "# submission_df.to_csv('submission.csv', header=False)\n",
    "\n",
    "# print(\"\\n完了: 'submission.csv'という名前で提出ファイルが作成されました。\")\n",
    "# print(\"提出ファイルの内容（先頭5行）:\")\n",
    "# print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# --- このステップの前提 ---\n",
    "# df_train: 全ての前処理が完了した訓練データ (DataFrame)\n",
    "# df_test: 全ての前処理が完了したテストデータ (DataFrame)\n",
    "# model: 学習済みのPyTorchモデル\n",
    "# device: 'cuda' or 'cpu'\n",
    "# TIMESTEPS = 24\n",
    "# TARGET_COLUMN = 'price_actual'\n",
    "\n",
    "# --- ステップ1: 訓練データの履歴をテストデータに結合 ---\n",
    "print(\"1. 訓練データの末尾をテストデータの履歴として結合中...\")\n",
    "\n",
    "# ★★★ 修正箇所 ★★★\n",
    "# df_trainから目的変数を除いた特徴量のみのスライスを取得\n",
    "history = df_train.drop(columns=[TARGET_COLUMN]).iloc[-TIMESTEPS:]\n",
    "\n",
    "# 助走期間とテストデータを結合\n",
    "# これで history と df_test の両方が356カラムとなり、正しく結合されます\n",
    "combined_test_data = pd.concat([history, df_test], ignore_index=False)\n",
    "\n",
    "print(f\"元のdf_testの形状: {df_test.shape}\")\n",
    "print(f\"助走期間を追加したデータの形状: {combined_test_data.shape}\")\n",
    "\n",
    "\n",
    "# --- ステップ2: 結合データから予測用シーケンスを作成 ---\n",
    "def create_test_sequences(input_data, timesteps):\n",
    "    X_list = []\n",
    "    X_data = input_data.values\n",
    "    for i in range(len(X_data) - timesteps):\n",
    "        X_list.append(X_data[i:i+timesteps])\n",
    "    return np.array(X_list)\n",
    "\n",
    "print(\"\\n2. 予測用のシーケンスを作成中...\")\n",
    "X_test_with_history = create_test_sequences(combined_test_data, TIMESTEPS)\n",
    "X_test_tensor = torch.tensor(X_test_with_history, dtype=torch.float32)\n",
    "\n",
    "print(f\"作成されたテストシーケンスの形状: {X_test_tensor.shape}\")\n",
    "assert len(X_test_tensor) == len(df_test)\n",
    "\n",
    "\n",
    "# --- ステップ3: モデルによる予測の実行 ---\n",
    "print(\"\\n3. モデルによる予測を実行中...\")\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs in test_loader:\n",
    "        inputs_tensor = inputs[0].to(device)\n",
    "        predictions = model(inputs_tensor)\n",
    "        all_predictions.append(predictions.cpu().numpy())\n",
    "\n",
    "predictions_np = np.concatenate(all_predictions).flatten()\n",
    "print(\"予測が完了しました。\")\n",
    "\n",
    "\n",
    "# --- ステップ4: 提出用CSVファイルの作成 ---\n",
    "print(\"\\n4. 提出用CSVファイルを作成中...\")\n",
    "\n",
    "submission_df = pd.DataFrame(\n",
    "    {'price_actual': predictions_np},\n",
    "    index=df_test.index\n",
    ")\n",
    "\n",
    "submission_df.to_csv('../submissions/06_09_submission_01.csv', header=False)\n",
    "\n",
    "print(\"\\n完了: '06_09_submission_01.csv'という名前で最終的な提出ファイルが作成されました。\")\n",
    "print(\"提出ファイルの内容（先頭5行）:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
