{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Notebookファイルからの相対パスでCSVファイルのパスを指定\n",
    "csv_file_path = '../data/train.csv'\n",
    "\n",
    "# ファイルの存在確認\n",
    "if not os.path.exists(csv_file_path):\n",
    "    print(f\"エラー: ファイルが見つかりません。パスを確認してください: {os.path.abspath(csv_file_path)}\")\n",
    "else:\n",
    "    try:\n",
    "        # CSVファイルをDataFrameに読み込む\n",
    "        # 'time'列 (0番目の列) をインデックスに指定し、同時に日付型としてパースする\n",
    "        df_train = pd.read_csv(csv_file_path, index_col='time', parse_dates=['time'])\n",
    "        # または index_col=0, parse_dates=[0] でも同じ結果になります。\n",
    "\n",
    "        print(\"DataFrameの読み込みと'time'列のインデックス化・日付型変換が完了しました。\")\n",
    "\n",
    "        # 読み込んだDataFrameの最初の5行を表示して確認\n",
    "        # print(\"\\nDataFrameの先頭5行:\")\n",
    "        # print(df_train.head())\n",
    "\n",
    "        # DataFrameのインデックス情報を確認\n",
    "        # print(\"\\nDataFrameのインデックス情報:\")\n",
    "        # print(df_train.index)\n",
    "\n",
    "        # DataFrameの基本情報を表示 (time列がインデックスになっていることを確認)\n",
    "        # print(\"\\nDataFrameの情報:\")\n",
    "        # df_train.info()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"エラー: CSVファイルの読み込み中に問題が発生しました: {e}\")\n",
    "\n",
    "# この時点で df_train は 'time' 列を DatetimeIndex として持っています。\n",
    "# この後の欠損値処理などは、この df_train に対して行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 欠損値補完 ---\n",
    "\n",
    "# 1. 0で補完するのが適切なカラム\n",
    "cols_to_fill_zero = [col for col in df_train.columns if 'rain' in col or 'snow' in col]\n",
    "cols_to_fill_zero.append('generation_solar')\n",
    "\n",
    "print(f\"0で補完するカラム: {cols_to_fill_zero}\")\n",
    "for col in cols_to_fill_zero:\n",
    "    if col in df_train.columns:\n",
    "        df_train[col] = df_train[col].fillna(0)\n",
    "\n",
    "# 2. 残りの連続変数に対して前方補完\n",
    "# continuous_colsは以前定義した連続変数のリスト\n",
    "remaining_continuous_cols = [col for col in continuous_cols if col not in cols_to_fill_zero]\n",
    "\n",
    "print(\"前方補完（ffill）を実行するカラム（一部抜粋）:\", remaining_continuous_cols[:5])\n",
    "for col in remaining_continuous_cols:\n",
    "    if col in df_train.columns:\n",
    "        df_train[col] = df_train[col].fillna(method='ffill')\n",
    "\n",
    "# 3. それでも先頭に残った欠損を後方補完（bfill）で埋める\n",
    "# （データの先頭から欠損が続いている場合、ffillでは埋まらないため）\n",
    "df_train = df_train.fillna(method='bfill')\n",
    "\n",
    "print(\"\\n欠損値の補完が完了しました。\")\n",
    "# 補完後に欠損がないか確認\n",
    "print(\"補完後の欠損値の数:\\n\", df_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# このコードを実行する前に、df_trainがロードされている必要があります。\n",
    "# 例: df_train = pd.read_csv('your_data.csv', index_col='time', parse_dates=True)\n",
    "\n",
    "# --- ステップ①: 風向をサイン・コサインに変換 ---\n",
    "print(\"ステップ①: 風向の処理を開始します...\")\n",
    "cities = ['valencia', 'madrid', 'bilbao', 'barcelona', 'seville']\n",
    "original_wind_deg_cols = []\n",
    "\n",
    "for city in cities:\n",
    "    wind_deg_col = f'{city}_wind_deg'\n",
    "    original_wind_deg_cols.append(wind_deg_col)\n",
    "\n",
    "    # 度数をラジアンに変換\n",
    "    radians = np.deg2rad(df_train[wind_deg_col])\n",
    "\n",
    "    # 新しいサイン・コサイン特徴量を作成\n",
    "    df_train[f'{city}_wind_deg_cos'] = np.cos(radians)\n",
    "    df_train[f'{city}_wind_deg_sin'] = np.sin(radians)\n",
    "\n",
    "# 元の風向きカラムを削除\n",
    "df_train = df_train.drop(columns=original_wind_deg_cols)\n",
    "print(f\"元の風向カラム {original_wind_deg_cols} を削除し、サイン・コサインカラムを10個追加しました。\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- ステップ②: 連続変数の標準化 ---\n",
    "print(\"ステップ②: 連続変数の標準化を開始します...\")\n",
    "\n",
    "# 以前の分類に基づく連続変数のリスト\n",
    "continuous_cols = [\n",
    "    'generation_biomass', 'generation_fossil_brown_coal/lignite', \n",
    "    'generation_fossil_gas', 'generation_fossil_hard_coal', 'generation_fossil_oil', \n",
    "    'generation_hydro_pumped_storage_consumption', 'generation_hydro_run_of_river_and_poundage', \n",
    "    'generation_hydro_water_reservoir', 'generation_nuclear', 'generation_other', \n",
    "    'generation_other_renewable', 'generation_solar', 'generation_waste', \n",
    "    'generation_wind_onshore', 'total_load_actual', 'price_actual'\n",
    "]\n",
    "for city in cities:\n",
    "    continuous_cols.extend([\n",
    "        f'{city}_temp', f'{city}_temp_min', f'{city}_temp_max', f'{city}_pressure', \n",
    "        f'{city}_humidity', f'{city}_wind_speed', f'{city}_rain_1h', f'{city}_rain_3h', \n",
    "        f'{city}_snow_3h', f'{city}_clouds_all'\n",
    "    ])\n",
    "\n",
    "# ステップ①で追加したサイン・コサインカラムもリストに追加\n",
    "for city in cities:\n",
    "    continuous_cols.extend([f'{city}_wind_deg_cos', f'{city}_wind_deg_sin'])\n",
    "\n",
    "# 欠損値があるとエラーになるため、対象カラムを絞り込み\n",
    "# （実際には、標準化の前に欠損値処理を行うのが望ましいです）\n",
    "cols_to_scale = [col for col in continuous_cols if col in df_train.columns and df_train[col].isnull().sum() == 0]\n",
    "print(f\"標準化の対象となるカラム数: {len(cols_to_scale)}\")\n",
    "\n",
    "# 標準化の実行\n",
    "scaler = StandardScaler()\n",
    "df_train[cols_to_scale] = scaler.fit_transform(df_train[cols_to_scale])\n",
    "\n",
    "print(\"連続変数の標準化が完了しました。\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 処理後のデータフレームの先頭を表示して確認\n",
    "print(\"処理後のデータフレームの先頭5行:\")\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# このコードを実行する前に、df_trainが前回の処理を終えた状態でロードされている必要があります。\n",
    "\n",
    "print(\"処理前のデータフレームの形状:\", df_train.shape)\n",
    "print(\"処理前のカラム（一部）:\", df_train.columns[:10].tolist(), \"...\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- ステップ①: weather_descriptionカラムを削除 ---\n",
    "print(\"ステップ①: weather_descriptionカラムの削除を開始します...\")\n",
    "cities = ['valencia', 'madrid', 'bilbao', 'barcelona', 'seville']\n",
    "description_cols_to_drop = [f'{city}_weather_description' for city in cities]\n",
    "\n",
    "# 存在するカラムのみを削除対象とする\n",
    "existing_cols_to_drop = [col for col in description_cols_to_drop if col in df_train.columns]\n",
    "if existing_cols_to_drop:\n",
    "    df_train = df_train.drop(columns=existing_cols_to_drop)\n",
    "    print(f\"削除されたカラム: {existing_cols_to_drop}\")\n",
    "else:\n",
    "    print(\"削除対象のdescriptionカラムは見つかりませんでした。\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- ステップ②: 残りのカテゴリ変数にOne-Hot Encodingを適用 ---\n",
    "print(\"ステップ②: One-Hot Encodingを開始します...\")\n",
    "\n",
    "# One-Hot Encodingの対象となるカラムのリストを作成\n",
    "categorical_cols_to_encode = []\n",
    "for city in cities:\n",
    "    categorical_cols_to_encode.extend([\n",
    "        f'{city}_weather_id',\n",
    "        f'{city}_weather_main',\n",
    "        f'{city}_weather_icon'\n",
    "    ])\n",
    "\n",
    "# df_trainに存在するカラムのみを対象とする\n",
    "existing_categorical_cols = [col for col in categorical_cols_to_encode if col in df_train.columns]\n",
    "\n",
    "if existing_categorical_cols:\n",
    "    # weather_idは数値型のため、カテゴリとして正しく扱われるように文字列に変換\n",
    "    for col in existing_categorical_cols:\n",
    "        if 'weather_id' in col:\n",
    "            df_train[col] = df_train[col].astype(str)\n",
    "    \n",
    "    print(f\"One-Hot Encoding対象のカラム: {existing_categorical_cols}\")\n",
    "    \n",
    "    # One-Hot Encodingを実行\n",
    "    # prefixを指定して、どの元のカラムから来た特徴量か分かりやすくします。\n",
    "    # 元のカテゴリカルカラムは自動的に削除されます。\n",
    "    df_train = pd.get_dummies(df_train, columns=existing_categorical_cols)\n",
    "    \n",
    "    print(\"One-Hot Encodingが完了しました。\")\n",
    "else:\n",
    "    print(\"One-Hot Encoding対象のカラムは見つかりませんでした。\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"全ての処理が完了しました。\")\n",
    "print(\"最終的なデータフレームの形状:\", df_train.shape)\n",
    "print(\"処理後のカラム（一部）:\", df_train.columns[:10].tolist(), \"...\")\n",
    "print(\"\\n処理後のデータフレームの先頭5行:\")\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"price_actual\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# df_train が前処理済みの状態で存在すると仮定します。\n",
    "# 例：df_train = pd.read_csv(...) などでロードされ、\n",
    "# 欠損値補完、標準化、One-Hotエンコーディングが完了している状態。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(input_data, target_column, timesteps):\n",
    "    \"\"\"\n",
    "    2Dの時系列データフレームから、LSTM用の3Dシーケンスデータを作成する関数\n",
    "    \"\"\"\n",
    "    X_list, y_list = [], []\n",
    "    \n",
    "    # 説明変数と目的変数に分離\n",
    "    # One-Hotエンコーディングにより目的変数の位置が変わっている可能性があるため、名前で指定\n",
    "    y = input_data[target_column].values\n",
    "    X_data = input_data.drop(columns=target_column).values\n",
    "    \n",
    "    for i in range(len(X_data) - timesteps):\n",
    "        X_list.append(X_data[i:i+timesteps])\n",
    "        y_list.append(y[i+timesteps])\n",
    "        \n",
    "    return np.array(X_list), np.array(y_list)\n",
    "\n",
    "# ハイパーパラメータ\n",
    "TIMESTEPS = 24\n",
    "TARGET_COLUMN = 'price_actual' # 目的変数のカラム名\n",
    "\n",
    "# ★★★ 修正箇所：ここから ★★★\n",
    "# bool型をint型に変換して、データ型を数値に統一します\n",
    "bool_cols = df_train.select_dtypes(include=['bool']).columns\n",
    "df_train[bool_cols] = df_train[bool_cols].astype(int)\n",
    "print(\"bool型をint型に変換しました。\")\n",
    "# ★★★ 修正箇所：ここまで ★★★\n",
    "\n",
    "print(\"シーケンスデータを作成中...\")\n",
    "X_seq, y_seq = create_sequences(df_train, TARGET_COLUMN, TIMESTEPS)\n",
    "\n",
    "print(f\"X_seq shape: {X_seq.shape}\") # -> (サンプル数, 24, 特徴量数)\n",
    "print(f\"y_seq shape: {y_seq.shape}\") # -> (サンプル数,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データと検証データの分割（例: 80%を訓練、20%を検証）\n",
    "train_size = int(len(X_seq) * 0.8)\n",
    "X_train, X_val = X_seq[:train_size], X_seq[train_size:]\n",
    "y_train, y_val = y_seq[:train_size], y_seq[train_size:]\n",
    "\n",
    "# PyTorchのテンソルに変換\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# DatasetとDataLoaderの作成\n",
    "BATCH_SIZE = 64 # バッチサイズも後で調整するハイパーパラメータ\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"DataLoaderの準備が完了しました。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_prob):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_prob if num_layers > 1 else 0 # 1層の場合はdropoutは適用されない\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM層からの出力を取得\n",
    "        # h_n, c_n (最後の隠れ状態とセル状態) は今回は使用しない\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # 最後のタイムステップの出力のみを選択\n",
    "        last_hidden_state = lstm_out[:, -1, :]\n",
    "        \n",
    "        # ドロップアウト層\n",
    "        out = self.dropout(last_hidden_state)\n",
    "        \n",
    "        # 全結合層（出力層）\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ハイパーパラメータ（Optunaで探索する候補）\n",
    "INPUT_SIZE = X_train.shape[2] # 特徴量の数\n",
    "HIDDEN_SIZE = 100\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT_PROB = 0.2\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 20 # まずは少ないエポック数で試す\n",
    "\n",
    "# デバイスの設定\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用デバイス: {device}\")\n",
    "\n",
    "# モデル、損失関数、オプティマイザのインスタンス化\n",
    "model = LSTMModel(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, DROPOUT_PROB).to(device)\n",
    "loss_fn = nn.MSELoss() # 平均二乗誤差\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"モデルの学習を開始します...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # --- 訓練モード ---\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    # --- 検証モード ---\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            predictions = model(inputs)\n",
    "            loss = loss_fn(predictions, labels)\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] | Train Loss: {avg_train_loss:.4f} | Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Notebookファイルからの相対パスでCSVファイルのパスを指定\n",
    "csv_file_path = '../data/test.csv'\n",
    "\n",
    "# ファイルの存在確認\n",
    "if not os.path.exists(csv_file_path):\n",
    "    print(f\"エラー: ファイルが見つかりません。パスを確認してください: {os.path.abspath(csv_file_path)}\")\n",
    "else:\n",
    "    try:\n",
    "        # CSVファイルをDataFrameに読み込む\n",
    "        # 'time'列 (0番目の列) をインデックスに指定し、同時に日付型としてパースする\n",
    "        df_test = pd.read_csv(csv_file_path, index_col='time', parse_dates=['time'])\n",
    "        # または index_col=0, parse_dates=[0] でも同じ結果になります。\n",
    "\n",
    "        print(\"DataFrameの読み込みと'time'列のインデックス化・日付型変換が完了しました。\")\n",
    "\n",
    "        # 読み込んだDataFrameの最初の5行を表示して確認\n",
    "        # print(\"\\nDataFrameの先頭5行:\")\n",
    "        # print(df_test.head())\n",
    "\n",
    "        # DataFrameのインデックス情報を確認\n",
    "        # print(\"\\nDataFrameのインデックス情報:\")\n",
    "        # print(df_test.index)\n",
    "\n",
    "        # DataFrameの基本情報を表示 (time列がインデックスになっていることを確認)\n",
    "        # print(\"\\nDataFrameの情報:\")\n",
    "        # df_test.info()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"エラー: CSVファイルの読み込み中に問題が発生しました: {e}\")\n",
    "\n",
    "# この時点で df_test は 'time' 列を DatetimeIndex として持っています。\n",
    "# この後の欠損値処理などは、この df_test に対して行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# このコードは、df_trainの前処理が完了し、\n",
    "# scaler オブジェクトと、最終的な df_train が存在することを前提とします。\n",
    "# df_test には目的変数 'price_actual' が含まれていないと仮定します。\n",
    "\n",
    "print(\"テストデータの前処理を開始します...\")\n",
    "print(f\"処理前のdf_testの形状: {df_test.shape}\")\n",
    "\n",
    "# --- ステップ1: 欠損値補完 ---\n",
    "# df_trainと同じ戦略で補完\n",
    "print(\"1. 欠損値を補完中...\")\n",
    "cols_to_fill_zero = [col for col in df_test.columns if 'rain' in col or 'snow' in col or 'solar' in col]\n",
    "df_test[cols_to_fill_zero] = df_test[cols_to_fill_zero].fillna(0)\n",
    "df_test = df_test.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# --- ステップ2: 風向の処理と不要カラムの削除 ---\n",
    "print(\"2. 風向をサイン・コサインに変換し、不要なカラムを削除中...\")\n",
    "cities = ['valencia', 'madrid', 'bilbao', 'barcelona', 'seville']\n",
    "\n",
    "# 風向の処理\n",
    "original_wind_deg_cols = [f'{city}_wind_deg' for city in cities]\n",
    "for col in original_wind_deg_cols:\n",
    "    if col in df_test.columns:\n",
    "        radians = np.deg2rad(df_test[col])\n",
    "        df_test[f'{col}_cos'] = np.cos(radians)\n",
    "        df_test[f'{col}_sin'] = np.sin(radians)\n",
    "df_test = df_test.drop(columns=original_wind_deg_cols)\n",
    "\n",
    "# weather_descriptionの削除\n",
    "description_cols_to_drop = [f'{city}_weather_description' for city in cities]\n",
    "df_test = df_test.drop(columns=description_cols_to_drop, errors='ignore')\n",
    "\n",
    "# --- ステップ3: 標準化 (訓練時のScalerを再利用) ---\n",
    "# df_trainの前処理で定義した continuous_cols リストが必要\n",
    "print(\"3. 訓練時のScalerを再利用して標準化中...\")\n",
    "\n",
    "# df_trainの処理で使用したcontinuous_colsのリストから、df_testに存在するカラムのみを対象にする\n",
    "# 'price_actual'はdf_testに存在しないため、自動的に除外される\n",
    "cols_to_scale_test = [col for col in continuous_cols if col in df_test.columns]\n",
    "\n",
    "# ★重要★ 訓練データで学習したscalerを使い、transformのみ適用\n",
    "df_test[cols_to_scale_test] = scaler.transform(df_test[cols_to_scale_test])\n",
    "\n",
    "\n",
    "# --- ステップ4: One-Hot Encoding とカラムの整合 ---\n",
    "print(\"4. One-Hot Encodingとカラムの整合性を確保中...\")\n",
    "\n",
    "# OHE対象カラムのリストを作成\n",
    "categorical_cols_to_encode = []\n",
    "for city in cities:\n",
    "    categorical_cols_to_encode.extend([f'{city}_weather_id', f'{city}_weather_main', f'{city}_weather_icon'])\n",
    "\n",
    "# 存在するカラムのみを対象とし、カテゴリとして扱うために文字列に変換\n",
    "existing_categorical_cols = [col for col in categorical_cols_to_encode if col in df_test.columns]\n",
    "for col in existing_categorical_cols:\n",
    "    df_test[col] = df_test[col].astype(str)\n",
    "\n",
    "# One-Hot Encodingを実行\n",
    "df_test = pd.get_dummies(df_test, columns=existing_categorical_cols)\n",
    "\n",
    "# ★重要★ df_trainのカラム構成にdf_testを完全に合わせる\n",
    "train_columns = df_train.drop(columns=[TARGET_COLUMN]).columns\n",
    "df_test = df_test.reindex(columns=train_columns, fill_value=0)\n",
    "\n",
    "# bool型をint型に変換\n",
    "bool_cols = df_test.select_dtypes(include=['bool']).columns\n",
    "df_test[bool_cols] = df_test[bool_cols].astype(int)\n",
    "\n",
    "\n",
    "print(\"\\n前処理が完了しました。\")\n",
    "print(f\"処理後のdf_testの形状: {df_test.shape}\")\n",
    "print(f\"比較（df_trainの形状）: ({df_train.shape[0]}, {df_train.shape[1]-1})\") # 目的変数を引いた数\n",
    "\n",
    "# カラムが一致しているか最終確認\n",
    "if len(train_columns) == len(df_test.columns) and all(train_columns == df_test.columns):\n",
    "    print(\"OK: df_trainとdf_testのカラム構成が一致しました。\")\n",
    "else:\n",
    "    print(\"NG: df_trainとdf_testのカラム構成が一致していません。見直しが必要です。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
